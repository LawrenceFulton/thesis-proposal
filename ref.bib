@article{le2019progress,
  title={Progress in Reducing Drink-Driving and Other Alcohol-Related Road Deaths in Europe},
  author={Le Li{\`e}vre, P and Adminaite, D and Jost, G and Podda, F},
  journal={European Transport Safety Council: Brussels, Belgium},
  year={2019}
}
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  bibliography file with collection of bib entries to cite
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{APA:2009:PubManual,
  title     = {{Publication Manual of the American Psychological Association}},
  edition   = 6,
  editor    = {{American~Psychological~Association}},
  keywords  = {6th APA Publication Manual},
  publisher = {American Psychological Association},
  year      = 2009,
  isbn      = {1433805618,9781433805615},
  url       = {https://apastyle.apa.org/6th-edition-resources},
}

@book{APA:PubManual:7ed,
  title     = {{Publication Manual of the American Psychological Association}},
  edition   = 7,
  year      = 2020,
  editor    = {Association, American Psychological},
  publisher = {American Psychological Association},
  url       = {https://apastyle.apa.org/products/publication-manual-7th-edition},
}


@inbook{10.1145/3477322.3477325,
  author    = {Rosenthal-von der P{\"u}tten, Astrid and Abrams, Anna M. H.},
  title     = {{Empirical Methods in the Social Science for Researching Socially Interactive Agents}},
  year      = {2021},
  isbn      = {9781450387200},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  edition   = {1},
  url       = {https://doi.org/10.1145/3477322.3477325},
  booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 1: Methods, Behavior, Cognition},
  pages     = {19--76},
  numpages  = {58},
}

@inproceedings{10.1145/3371382.3374852,
author = {Rosenthal-von der P\"{u}tten, Astrid and Sirkin, David and Abrams, Anna and Platte, Laura},
title = {The Forgotten in HRI: Incidental Encounters with Robots in Public Spaces},
year = {2020},
isbn = {9781450370578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371382.3374852},
doi = {10.1145/3371382.3374852},
abstract = {HRI research has predominantly focused on laboratory studies, producing a fundamental understanding of how humans interact with robots in controlled settings. As robots transition out of research and development labs into the real world, HRI research must adapt. We argue that it should widen its scope to explicitly include people who do not deliberately seek an interaction with a robot (users) but find themselves in coincidental presence with robots. We refer to this often-forgotten group as InCoPs (incidentally copresent persons). In this one-day workshop, we aim to explore studies, design approaches, and methodologies for testing robots in real-world environments, considering both users and InCoPs. The first part of the workshop will consist of invited talks addressing the subject from different angles, followed by plenary discussions. Building upon this common basis, participants will work in small groups to explore (1) human behavior, (2) robot and interaction design and (3) methodology, respectively. This group phase will focus on the exemplary scenario of delivery robots in urban environments. At the end, key aspects across all three topics will be identified and discussed to map out research needs and desirable next steps in the field.},
booktitle = {Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
  pages = {656--657},
  numpages = {2},
  keywords = {human-robot interaction, real-world interaction, incops, incidentally copresent persons},
  location = {Cambridge, United Kingdom},
  series = {HRI '20}
}



@article{gallegos_bias_2024,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={Computational Linguistics},
  volume={50},
  number={3},
  pages={1097--1179},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@inproceedings{sancheti_llm_2024,
	location = {Singapore Singapore},
	title = {{LLM} Driven Web Profile Extraction for Identical Names},
	isbn = {979-8-4007-0172-6},
	url = {https://dl.acm.org/doi/10.1145/3589335.3651946},
	doi = {10.1145/3589335.3651946},
	abstract = {The number of individuals having identical names on the internet is increasing. Thus making the task of searching for a specific individual tedious. The user must vet through many profiles with identical names to get to the actual individual of interest. The online presence of an individual forms the profile of the individual. We need a solution that helps users by consolidating the profiles of such individuals by retrieving factual information available on the web and providing the same as a single result. We present a novel solution that retrieves web profiles belonging to those bearing identical Full Names through an end-to-end pipeline. Our solution involves information retrieval from the web (extraction), {LLM}-driven Named Entity Extraction (retrieval), and standardization of facts using Wikipedia, which returns profiles with fourteen multi-valued attributes. After that, profiles that correspond to the same real-world individuals are determined. We accomplish this by identifying similarities among profiles based on the extracted facts using a Prefix Tree inspired data structure (validation) and utilizing {ChatGPT}’s contextual comprehension (revalidation). The system offers varied levels of strictness while consolidating these profiles, namely strict, relaxed, and loose matching. The novelty of our solution lies in the innovative use of {GPT} – a highly powerful yet an unpredictable tool, for such a nuanced task. A study involving twenty participants, along with other results, found that one could effectively retrieve information for a specific individual.},
	eventtitle = {{WWW} '24: The {ACM} Web Conference 2024},
	pages = {1616--1625},
	booktitle = {Companion Proceedings of the {ACM} Web Conference 2024},
	publisher = {{ACM}},
	author = {Sancheti, Prateek and Karlapalem, Kamalakar and Vemuri, Kavita},
	urldate = {2025-02-08},
	date = {2024-05-13},
	langid = {english},
	file = {PDF:/Users/lawrencefulton/Zotero/storage/S9PHTTTQ/Sancheti et al. - 2024 - LLM Driven Web Profile Extraction for Identical Names.pdf:application/pdf},
}

@misc{li_multi-step_2023,
	title = {Multi-step Jailbreaking Privacy Attacks on {ChatGPT}},
	url = {http://arxiv.org/abs/2304.05197},
	doi = {10.48550/arXiv.2304.05197},
	abstract = {With the rapid progress of large language models ({LLMs}), many downstream {NLP} tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from {LLMs}, it is still challenging to steer {AI}-generated content ({AIGC}) for the human good. As powerful {LLMs} are devouring existing text data from various domains (e.g., {GPT}-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these {LLMs} and their downstream applications bring. In this paper, we study the privacy threats from {OpenAI}’s {ChatGPT} and the New Bing enhanced by {ChatGPT} and show that application-integrated {LLMs} may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss {LLMs}’ privacy implications.},
	number = {{arXiv}:2304.05197},
	publisher = {{arXiv}},
	author = {Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Huang, Jie and Meng, Fanpu and Song, Yangqiu},
	urldate = {2025-02-08},
	date = {2023-11-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.05197 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, {PII}},
	file = {PDF:/Users/lawrencefulton/Zotero/storage/Q2Y4PHIF/Li et al. - 2023 - Multi-step Jailbreaking Privacy Attacks on ChatGPT.pdf:application/pdf},
}

@misc{schwartzman_exfiltration_2024,
	title = {Exfiltration of personal information from {ChatGPT} via prompt injection},
	url = {http://arxiv.org/abs/2406.00199},
	doi = {10.48550/arXiv.2406.00199},
	abstract = {We report that {ChatGPT} 4 and 4o are susceptible to a prompt injection attack that allows an attacker to exﬁltrate users’ personal data. It is applicable without the use of any 3rd party tools and all users are currently aﬀected. This vulnerability is exacerbated by the recent introduction of {ChatGPT}’s memory feature, which allows an attacker to command {ChatGPT} to monitor the user for the desired personal data.},
	number = {{arXiv}:2406.00199},
	publisher = {{arXiv}},
	author = {Schwartzman, Gregory},
	urldate = {2025-02-08},
	date = {2024-06-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.00199 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Emerging Technologies},
	file = {PDF:/Users/lawrencefulton/Zotero/storage/4YDADC4T/Schwartzman - 2024 - Exfiltration of personal information from ChatGPT via prompt injection.pdf:application/pdf},
}

@inproceedings{lukas_analyzing_2023,
	title = {Analyzing Leakage of Personally Identifiable Information in Language Models},
	url = {https://ieeexplore.ieee.org/abstract/document/10179300?casa_token=FoZHenJ4KbYAAAAA:u1wY2vJ3tXEZ9PxU4fBgocuULd6O8ep1wtUXeeGvI9f8D5dYelY4yz2z2ur-WAxv8wtqD-FIVA},
	doi = {10.1109/SP46215.2023.10179300},
	abstract = {Language Models ({LMs}) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of {LMs} leaking Personally Identifiable Information ({PII}) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent {PII} leakage. Scrubbing techniques reduce but do not prevent the risk of {PII} leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence-or user-level privacy, prevent {PII} disclosure. In this work, we introduce rigorous game-based definitions for three types of {PII} leakage via black-box extraction, inference, and reconstruction attacks with only {API} access to an {LM}. We empirically evaluate the attacks against {GPT}-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10× more {PII} sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of {PII} disclosure but still leaks about 3\% of {PII} sequences, and (iii) a subtle connection between record-level membership inference and {PII} reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing\_pii\_leakage.},
	eventtitle = {2023 {IEEE} Symposium on Security and Privacy ({SP})},
	pages = {346--363},
	booktitle = {2023 {IEEE} Symposium on Security and Privacy ({SP})},
	author = {Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-Béguelin, Santiago},
	urldate = {2025-02-13},
	date = {2023-05},
	note = {{ISSN}: 2375-1207},
	keywords = {Analytical models, Data privacy, Data-Extraction, Data-Reconstruction, Differential privacy, Differential-Privacy, Language-Models, Personally-Identifiable-Information, Pipelines, Privacy, Scrubbing, Training, Training data},
	file = {Full Text PDF:/Users/lawrencefulton/Zotero/storage/AP82WXAB/Lukas et al. - 2023 - Analyzing Leakage of Personally Identifiable Information in Language Models.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/lawrencefulton/Zotero/storage/DWLBAGUJ/10179300.html:text/html},
}

@article{nakka_pii-compass_2024,
	title = {{PII}-Compass: Guiding {LLM} training data extraction prompts towards the target {PII} via grounding},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2407.02943},
	doi = {10.48550/ARXIV.2407.02943},
	shorttitle = {{PII}-Compass},
	abstract = {The latest and most impactful advances in large models stem from their increased size. Unfortunately, this translates into an improved memorization capacity, raising data privacy concerns. Specifically, it has been shown that models can output personal identifiable information ({PII}) contained in their training data. However, reported {PIII} extraction performance varies widely, and there is no consensus on the optimal methodology to evaluate this risk, resulting in underestimating realistic adversaries. In this work, we empirically demonstrate that it is possible to improve the extractability of {PII} by over ten-fold by grounding the prefix of the manually constructed extraction prompt with in-domain data. Our approach, {PII}-Compass, achieves phone number extraction rates of 0.92\%, 3.9\%, and 6.86\% with 1, 128, and 2308 queries, respectively, i.e., the phone number of 1 person in 15 is extractable.},
	author = {Nakka, Krishna Kanth and Frikha, Ahmed and Mendes, Ricardo and Jiang, Xue and Zhou, Xuebing},
	urldate = {2025-02-19},
	date = {2024},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), Cryptography and Security (cs.{CR}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
	file = {Full Text PDF:/Users/lawrencefulton/Zotero/storage/5H4AAAG4/Nakka et al. - 2024 - PII-Compass Guiding LLM training data extraction prompts towards the target PII via grounding.pdf:application/pdf},
}

@misc{shao_quantifying_2024,
	title = {Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage},
	url = {http://arxiv.org/abs/2305.12707},
	doi = {10.48550/arXiv.2305.12707},
	abstract = {The advancement of large language models ({LLMs}) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of {LLMs} is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information ({PII}). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus {PII}, with the latter showing lower accuracy. Despite the proportion of accurately predicted {PII} being relatively small, {LLMs} still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. These findings underscore the potential risk to {PII} confidentiality posed by the evolving capabilities of {LLMs}, especially as they continue to expand in scale and power.},
	number = {{arXiv}:2305.12707},
	publisher = {{arXiv}},
	author = {Shao, Hanyin and Huang, Jie and Zheng, Shen and Chang, Kevin Chen-Chuan},
	urldate = {2025-02-19},
	date = {2024-02-09},
	eprinttype = {arxiv},
	eprint = {2305.12707 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/lawrencefulton/Zotero/storage/RR3WJJPA/Shao et al. - 2024 - Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakag.pdf:application/pdf;Snapshot:/Users/lawrencefulton/Zotero/storage/MVED58DK/2305.html:text/html},
}

@report{holtdirk_fine-tuning_2024,
	title = {Fine-Tuning Large Language Models to Simulate German Voting Behaviour},
	url = {https://osf.io/download/6702c880f240037f8fa2009a/},
	institution = {Center for Open Science},
	author = {Holtdirk, Tobias and Assenmacher, Dennis and Bleier, Arnim and Wagner, Claudia},
	urldate = {2025-02-19},
	date = {2024},
	file = {Available Version (via Google Scholar):/Users/lawrencefulton/Zotero/storage/YM25YR5I/Holtdirk et al. - 2024 - Fine-Tuning Large Language Models to Simulate German Voting Behaviour.pdf:application/pdf},
}

@article{taubenfeld_systematic_2024,
  title={Systematic biases in LLM simulations of debates},
  author={Taubenfeld, Amir and Dover, Yaniv and Reichart, Roi and Goldstein, Ariel},
  journal={arXiv preprint arXiv:2402.04049},
  year={2024}
}


@misc{coppolillo_unmasking_2025,
	title = {Unmasking Conversational Bias in {AI} Multiagent Systems},
	url = {http://arxiv.org/abs/2501.14844},
	doi = {10.48550/arXiv.2501.14844},
	abstract = {Detecting biases in the outputs produced by generative models is essential to reduce the potential risks associated with their application in critical settings. However, the majority of existing methodologies for identifying biases in generated text consider the models in isolation and neglect their contextual applications. Specifically, the biases that may arise in multi-agent systems involving generative models remain under-researched. To address this gap, we present a framework designed to quantify biases within multi-agent systems of conversational Large Language Models ({LLMs}). Our approach involves simulating small echo chambers, where pairs of {LLMs}, initialized with aligned perspectives on a polarizing topic, engage in discussions. Contrary to expectations, we observe significant shifts in the stance expressed in the generated messages, particularly within echo chambers where all agents initially express conservative viewpoints, in line with the well-documented political bias of many {LLMs} toward liberal positions. Crucially, the bias observed in the echo-chamber experiment remains undetected by current state-of-the-art bias detection methods that rely on questionnaires. This highlights a critical need for the development of a more sophisticated toolkit for bias detection and mitigation for {AI} multi-agent systems. The code to perform the experiments is publicly available at https://anonymous.4open.science/r/{LLMsConversationalBias}-7725.},
	number = {{arXiv}:2501.14844},
	publisher = {{arXiv}},
	author = {Coppolillo, Erica and Manco, Giuseppe and Aiello, Luca Maria},
	urldate = {2025-02-25},
	date = {2025-02-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2501.14844 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems},
	file = {PDF:/Users/lawrencefulton/Zotero/storage/3W3Q7JZD/Coppolillo et al. - 2025 - Unmasking Conversational Bias in AI Multiagent Systems.pdf:application/pdf},
}

@article{tessler_ai_2024,
	title = {{AI} can help humans find common ground in democratic deliberation},
	volume = {386},
	url = {https://www.science.org/doi/10.1126/science.adq2852},
	doi = {10.1126/science.adq2852},
	abstract = {Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence ({AI}) to mediate human deliberation. Using participants’ personal opinions and critiques, the {AI} mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred {AI}-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the {UK} population.},
	pages = {eadq2852},
	number = {6719},
	journaltitle = {Science},
	author = {Tessler, Michael Henry and Bakker, Michiel A. and Jarrett, Daniel and Sheahan, Hannah and Chadwick, Martin J. and Koster, Raphael and Evans, Georgina and Campbell-Gillingham, Lucy and Collins, Tantum and Parkes, David C. and Botvinick, Matthew and Summerfield, Christopher},
	urldate = {2025-02-26},
	date = {2024-10-18},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Full Text PDF:/Users/lawrencefulton/Zotero/storage/S4ZC68FW/Tessler et al. - 2024 - AI can help humans find common ground in democratic deliberation.pdf:application/pdf},
}

@misc{chuang_simulating_2024,
	title = {Simulating Opinion Dynamics with Networks of {LLM}-based Agents},
	url = {http://arxiv.org/abs/2311.09618},
	doi = {10.48550/arXiv.2311.09618},
	abstract = {Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models ({ABMs}) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models ({LLMs}). Our findings reveal a strong inherent bias in {LLM} agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of {LLM} agents in this domain and suggest a path forward: refining {LLMs} with real-world discourse to better simulate the evolution of human beliefs.},
	number = {{arXiv}:2311.09618},
	publisher = {{arXiv}},
	author = {Chuang, Yun-Shiuan and Goyal, Agam and Harlalka, Nikunj and Suresh, Siddharth and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T.},
	urldate = {2025-03-04},
	date = {2024-04-01},
	eprinttype = {arxiv},
	eprint = {2311.09618 [physics]},
	keywords = {Computer Science - Computation and Language, Physics - Physics and Society},
	file = {Preprint PDF:/Users/lawrencefulton/Zotero/storage/FC6UFJB8/Chuang et al. - 2024 - Simulating Opinion Dynamics with Networks of LLM-based Agents.pdf:application/pdf;Snapshot:/Users/lawrencefulton/Zotero/storage/ME9K7CBL/2311.html:text/html},
}

@article{nikolova_men_2016,
	title = {Men and the Middle: Gender Differences in Dyadic Compromise Effects},
	volume = {43},
	issn = {0093-5301, 1537-5277},
	url = {https://academic.oup.com/jcr/article-lookup/doi/10.1093/jcr/ucw035},
	doi = {10.1093/jcr/ucw035},
	shorttitle = {Men and the Middle},
	abstract = {Individual decision makers show robust tendencies toward choosing compromise options. But what happens when consumers make choices with someone else? This article examines the choice of compromise options in joint dyadic decisions. Findings reveal that preferences for compromise alternatives replicate in mixedgender and female-female dyads as among individuals but are attenuated when two males make a choice together. Moreover, when two males make joint choices, their tendency to choose the compromise alternative decreases not only relative to other types of pairs but also to male and female individual decision makers. Evidence is presented that this happens because male-male dyadic contexts cue gender dichotomization, behavior that is consistent with masculine but not feminine gender norms. Because the extremity in decision making is maximally consistent with masculine but not feminine gender role norms, male-male dyads exhibit lower preferences for compromise options. However, if men have an opportunity to signal masculinity to one another prior to making joint compromise choices, male-male dyads prefer compromise options at proportions no different from female-female dyads. This work brings together the judgment and decisionmaking literature with insights from the social psychology literature, identifying a case when gender role norms have profound inﬂuences on classic judgment and decision-making effects.},
	pages = {355--371},
	number = {3},
	journaltitle = {Journal of Consumer Research},
	shortjournal = {J Consum Res},
	author = {Nikolova, Hristina and Lamberton, Cait},
	urldate = {2025-03-07},
	date = {2016-10},
	langid = {english},
	file = {PDF:/Users/lawrencefulton/Zotero/storage/J9TJGGIB/ucw035.pdf:application/pdf},
}
@article{bowles2010gender,
  title={Gender and persistence in negotiation: A dyadic perspective},
  author={Bowles, Hannah Riley and Flynn, Francis},
  journal={Academy of Management Journal},
  volume={53},
  number={4},
  pages={769--787},
  year={2010},
  publisher={Academy of Management Briarcliff Manor, NY}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  note={Accessed: 2025-03-20},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@article{min2023recent,
  title={Recent advances in natural language processing via large pre-trained language models: A survey},
  author={Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
  journal={ACM Computing Surveys},
  volume={56},
  number={2},
  pages={1--40},
  year={2023},
  publisher={ACM New York, NY}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}


@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}


@article{van2020survey,
  title={A survey on semi-supervised learning},
  author={Van Engelen, Jesper E and Hoos, Holger H},
  journal={Machine learning},
  volume={109},
  number={2},
  pages={373--440},
  year={2020},
  publisher={Springer}
}


@inproceedings{yarowsky1995unsupervised,
  title={Unsupervised word sense disambiguation rivaling supervised methods},
  author={Yarowsky, David},
  booktitle={33rd annual meeting of the association for computational linguistics},
  pages={189--196},
  year={1995}
}


@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}




@article{kimDHR17,
  author       = {Yoon Kim and
                  Carl Denton and
                  Luong Hoang and
                  Alexander M. Rush},
  title        = {Structured Attention Networks},
  journal      = {CoRR},
  volume       = {abs/1702.00887},
  year         = {2017},
  url          = {http://arxiv.org/abs/1702.00887},
  eprinttype    = {arXiv},
  eprint       = {1702.00887},
  timestamp    = {Mon, 13 Aug 2018 16:47:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KimDHR17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}



@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{kenton2021alignment,
  title={Alignment of language agents},
  author={Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2103.14659},
  year={2021}
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{karimi2018homophily,
  title={Homophily influences ranking of minorities in social networks},
  author={Karimi, Fariba and G{\'e}nois, Mathieu and Wagner, Claudia and Singer, Philipp and Strohmaier, Markus},
  journal={Scientific reports},
  volume={8},
  number={1},
  pages={11077},
  year={2018},
  publisher={Nature Publishing Group UK London}
}
@article{lerman2014leveraging,
  title={Leveraging position bias to improve peer recommendation},
  author={Lerman, Kristina and Hogg, Tad},
  journal={PloS one},
  volume={9},
  number={6},
  pages={e98914},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}


@article{hill2013wikipedia,
  title={The Wikipedia gender gap revisited: Characterizing survey response bias with propensity score estimation},
  author={Hill, Benjamin Mako and Shaw, Aaron},
  journal={PloS one},
  volume={8},
  number={6},
  pages={e65782},
  year={2013},
  publisher={Public Library of Science San Francisco, USA}
}

@article{goldfarb2023prompt,
  title={This prompt is measuring<MASK>: Evaluating Bias Evaluation in Language Models},
  author={Goldfarb-Tarrant, Seraphina and Ungless, Eddie and Balkir, Esma and Blodgett, Su Lin},
  journal={arXiv preprint arXiv:2305.12757},
  year={2023}
}

@inproceedings{goldfarb2021intrinsic,
    title = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    author = "Goldfarb-Tarrant, Seraphina  and
      Marchant, Rebecca  and
      Mu{\~n}oz S{\'a}nchez, Ricardo  and
      Pandya, Mugdha  and
      Lopez, Adam",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.150/",
    doi = "10.18653/v1/2021.acl-long.150",
    pages = "1926--1940"
}
@inproceedings{cao2021holistic,
    title = "Holistic interpretation in locative alternation {--} Evidence from self-paced reading",
    author = "Cao, Rui",
    editor = "Hu, Kaibao  and
      Kim, Jong-Bok  and
      Zong, Chengqing  and
      Chersoni, Emmanuele",
    booktitle = "Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation",
    month = "11",
    year = "2021",
    address = "Shanghai, China",
    publisher = "Association for Computational Lingustics",
    url = "https://aclanthology.org/2021.paclic-1.57/",
    pages = "543--550"
}

@article{may2019measuring,
  title={On measuring social biases in sentence encoders},
  author={May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R and Rudinger, Rachel},
  journal={arXiv preprint arXiv:1903.10561},
  year={2019}
}

@article{dolci2023improving,
  title={Improving gender-related fairness in sentence encoders: A semantics-based approach},
  author={Dolci, Tommaso and Azzalini, Fabio and Tanelli, Mara},
  journal={Data Science and Engineering},
  volume={8},
  number={2},
  pages={177--195},
  year={2023},
  publisher={Springer}
}
@article{caliskan2017semantics,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
  journal={Science},
  volume={356},
  number={6334},
  pages={183--186},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{webster2020measuring,
  title={Measuring and reducing gendered correlations in pre-trained models},
  author={Webster, Kellie and Wang, Xuezhi and Tenney, Ian and Beutel, Alex and Pitler, Emily and Pavlick, Ellie and Chen, Jilin and Chi, Ed and Petrov, Slav},
  journal={arXiv preprint arXiv:2010.06032},
  year={2020}
}
@article{kurita2019measuring,
  title={Measuring bias in contextualized word representations},
  author={Kurita, Keita and Vyas, Nidhi and Pareek, Ayush and Black, Alan W and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:1906.07337},
  year={2019}
}

@article{kaneko2022debiasing,
  title={Debiasing isn't enough!--On the Effectiveness of Debiasing MLMs and their Social Biases in Downstream Tasks},
  author={Kaneko, Masahiro and Bollegala, Danushka and Okazaki, Naoaki},
  journal={arXiv preprint arXiv:2210.02938},
  year={2022}
}


@inproceedings{delobelle2022measuring,
  title={Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models},
  author={Delobelle, Pieter and Tokpo, Ewoenam Kwaku and Calders, Toon and Berendt, Bettina},
  booktitle={Proceedings of the 2022 Conference of the North American chapter of the association for computational linguistics},
  pages={1693--1706},
  year={2022},
  organization={Association for Computational Linguistics}
}

@article{bordia2019identifying,
  title={Identifying and reducing gender bias in word-level language models},
  author={Bordia, Shikha and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1904.03035},
  year={2019}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}


@article{cheng2023marked,
  title={Marked personas: Using natural language prompts to measure stereotypes in language models},
  author={Cheng, Myra and Durmus, Esin and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2305.18189},
  year={2023}
}


@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}


@article{huang2019reducing,
  title={Reducing sentiment bias in language models via counterfactual evaluation},
  author={Huang, Po-Sen and Zhang, Huan and Jiang, Ray and Stanforth, Robert and Welbl, Johannes and Rae, Jack and Maini, Vishal and Yogatama, Dani and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1911.03064},
  year={2019}
}

@article{sheng2019woman,
  title={The woman worked as a babysitter: On biases in language generation},
  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
  journal={arXiv preprint arXiv:1909.01326},
  year={2019}
}


@inproceedings{samory2021call,
  title={“Call me sexist, but...”: Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples},
  author={Samory, Mattia and Sen, Indira and Kohne, Julian and Fl{\"o}ck, Fabian and Wagner, Claudia},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={15},
  pages={573--584},
  year={2021}
}

@misc{PerspectiveAPI2024,
  author = {Jigsaw and Google},
  title = {Perspective API},
  year = {2024},
  url = {https://perspectiveapi.com}
}

@inproceedings{nozza2021honest,
  title={HONEST: Measuring hurtful sentence completion in language models},
  author={Nozza, Debora and Bianchi, Federico and Hovy, Dirk and others},
  booktitle={Proceedings of the 2021 conference of the North American chapter of the association for computational linguistics: Human language technologies},
  year={2021},
  organization={Association for Computational Linguistics}
}
@inproceedings{dhamala2021bold,
  title={Bold: Dataset and metrics for measuring biases in open-ended language generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={862--872},
  year={2021}
}
@article{gligoric2024nlp,
  title={NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps},
  author={Gligoric, Kristina and Cheng, Myra and Zheng, Lucia and Durmus, Esin and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2404.01651},
  year={2024}
}

@book{shoham2008multiagent,
  title={Multiagent systems: Algorithmic, game-theoretic, and logical foundations},
  author={Shoham, Yoav and Leyton-Brown, Kevin},
  year={2008},
  publisher={Cambridge University Press}
}
@book{wooldridge2009introduction,
  title={An introduction to multiagent systems},
  author={Wooldridge, Michael},
  year={2009},
  publisher={John wiley \& sons}
}

@inproceedings{stone2010ad,
  title={Ad hoc autonomous agent teams: Collaboration without pre-coordination},
  author={Stone, Peter and Kaminka, Gal and Kraus, Sarit and Rosenschein, Jeffrey},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={24},
  number={1},
  pages={1504--1509},
  year={2010}
}

@inproceedings{baker2019emergent,
  title={Emergent tool use from multi-agent autocurricula},
  author={Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
  booktitle={International conference on learning representations},
  year={2019}
}
@article{kaiya2023lyfe,
  title={Lyfe agents: Generative agents for low-cost real-time social interactions},
  author={Kaiya, Zhao and Naim, Michelangelo and Kondic, Jovana and Cortes, Manuel and Ge, Jiaxin and Luo, Shuying and Yang, Guangyu Robert and Ahn, Andrew},
  journal={arXiv preprint arXiv:2310.02172},
  year={2023}
}
@article{zhao2023competeai,
  title={Competeai: Understanding the competition dynamics in large language model-based agents},
  author={Zhao, Qinlin and Wang, Jindong and Zhang, Yixuan and Jin, Yiqiao and Zhu, Kaijie and Chen, Hao and Xie, Xing},
  journal={arXiv preprint arXiv:2310.17512},
  year={2023}
}

@article{charness2002understanding,
  title={Understanding social preferences with simple tests},
  author={Charness, Gary and Rabin, Matthew},
  journal={The quarterly journal of economics},
  volume={117},
  number={3},
  pages={817--869},
  year={2002},
  publisher={MIT Press}
}


@article{samuelson1988status,
  title={Status quo bias in decision making},
  author={Samuelson, William and Zeckhauser, Richard},
  journal={Journal of risk and uncertainty},
  volume={1},
  pages={7--59},
  year={1988},
  publisher={Springer}
}

@article{cui2024can,
  title={Can AI Replace Human Subjects? A Large-Scale Replication of Psychological Experiments with LLMs},
  author={Cui, Ziyan and Li, Ning and Zhou, Huaikang},
  journal={arXiv preprint arXiv:2409.00128},
  year={2024}
}

@inproceedings{aher2023using,
  title={Using large language models to simulate multiple humans and replicate human subject studies},
  author={Aher, Gati V and Arriaga, Rosa I and Kalai, Adam Tauman},
  booktitle={International Conference on Machine Learning},
  pages={337--371},
  year={2023},
  organization={PMLR}
}

@article{terren2021echo,
  title={Echo chambers on social media: A systematic review of the literature},
  author={Terren, Ludovic Terren Ludovic and Borge-Bravo, Rosa Borge-Bravo Rosa},
  journal={Review of Communication Research},
  volume={9},
  year={2021}
}
@article{hobolt2024polarizing,
  title={The polarizing effect of partisan echo chambers},
  author={Hobolt, Sara B and Lawall, Katharina and Tilley, James},
  journal={American Political Science Review},
  volume={118},
  number={3},
  pages={1464--1479},
  year={2024},
  publisher={Cambridge University Press}
}

@article{neuberger2024sauce,
  title={SAUCE: Synchronous and Asynchronous User-Customizable Environment for Multi-Agent LLM Interaction},
  author={Neuberger, Shlomo and Eckhaus, Niv and Berger, Uri and Taubenfeld, Amir and Stanovsky, Gabriel and Goldstein, Ariel},
  journal={arXiv preprint arXiv:2411.03397},
  year={2024}
}


@article{kray2001battle,
  title={Battle of the sexes: gender stereotype confirmation and reactance in negotiations.},
  author={Kray, Laura J and Thompson, Leigh and Galinsky, Adam},
  journal={Journal of personality and social psychology},
  volume={80},
  number={6},
  pages={942},
  year={2001},
  publisher={American Psychological Association}
}
@article{rutinowski2024self,
  title={The self-perception and political biases of ChatGPT},
  author={Rutinowski, J{\'e}r{\^o}me and Franke, Sven and Endendyk, Jan and Dormuth, Ina and Roidl, Moritz and Pauly, Markus},
  journal={Human Behavior and Emerging Technologies},
  volume={2024},
  number={1},
  pages={7115633},
  year={2024},
  publisher={Wiley Online Library}
}

@article{rettenberger2025assessing,
  title={Assessing political bias in large language models},
  author={Rettenberger, Luca and Reischl, Markus and Schutera, Mark},
  journal={Journal of Computational Social Science},
  volume={8},
  number={2},
  pages={1--17},
  year={2025},
  publisher={Springer}
}

@article{li2024language,
  title={Language Ranker: A Metric for Quantifying LLM Performance Across High and Low-Resource Languages},
  author={Li, Zihao and Shi, Yucheng and Liu, Zirui and Yang, Fan and Payani, Ali and Liu, Ninghao and Du, Mengnan},
  journal={arXiv preprint arXiv:2404.11553},
  year={2024}
}
@article{gupta2023bias,
  title={Bias runs deep: Implicit reasoning biases in persona-assigned llms},
  author={Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande, Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal, Ashish and Khot, Tushar},
  journal={arXiv preprint arXiv:2311.04892},
  year={2023}
}

@article{sclar2023quantifying,
  title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
  author={Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
  journal={arXiv preprint arXiv:2310.11324},
  year={2023}
}

@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}

@article{jiang2020can,
  title={How can we know what language models know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{chatterjee2024posix,
  title={POSIX: A Prompt Sensitivity Index For Large Language Models},
  author={Chatterjee, Anwoy and Renduchintala, HSVNS Kowndinya and Bhatia, Sumit and Chakraborty, Tanmoy},
  journal={arXiv preprint arXiv:2410.02185},
  year={2024}
}

@article{zhuo2024prosa,
  title={ProSA: Assessing and understanding the prompt sensitivity of LLMs},
  author={Zhuo, Jingming and Zhang, Songyang and Fang, Xinyu and Duan, Haodong and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2410.12405},
  year={2024}
}

@article{von2024vox,
  title={Vox populi, vox ai? using language models to estimate german public opinion},
  author={von der Heyde, Leah and Haensch, Anna-Carolina and Wenz, Alexander},
  journal={arXiv preprint arXiv:2407.08563},
  year={2024}
}

@book{cohen2013statistical,
  title={Statistical power analysis for the behavioral sciences},
  author={Cohen, Jacob},
  year={1988},
  publisher={Routledge},
  doi = {https://doi.org/10.4324/9780203771587},
}

@inproceedings{yin2024should,
  title={Should we respect LLMs? A cross-lingual study on the influence of prompt politeness on LLM performance},
  author={Yin, Ziqi and Wang, Hao and Horio, Kaito and Kawahara, Daisuike and Sekine, Satoshi},
  booktitle={Proceedings of the Second Workshop on Social Influence in Conversations (SICon 2024)},
  pages={9--35},
  year={2024}
}
